<!DOCTYPE html>















<html class="theme-next muse use-motion" lang="zh-Hans">

<head>
  <meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
  <meta name="theme-color" content="#222">
























  <link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

  <link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







  <script id="hexo.configurations">
    var NexT = window.NexT || {};
    var CONFIG = {
      root: '/',
      scheme: 'Muse',
      version: '7.1.0',
      sidebar: {
        "position": "left",
        "display": "post",
        "offset": 12,
        "onmobile": false,
        "dimmer": false
      },
      back2top: true,
      back2top_sidebar: false,
      fancybox: false,
      fastclick: false,
      lazyload: false,
      tabs: true,
      motion: {
        "enable": true,
        "async": false,
        "transition": {
          "post_block": "fadeIn",
          "post_header": "slideDownIn",
          "post_body": "slideDownIn",
          "coll_header": "slideLeftIn",
          "sidebar": "slideUpIn"
        }
      },
      algolia: {
        applicationID: '',
        apiKey: '',
        indexName: '',
        hits: {
          "per_page": 10
        },
        labels: {
          "input_placeholder": "Search for Posts",
          "hits_empty": "We didn't find any results for the search: ${query}",
          "hits_stats": "${hits} results found in ${time} ms"
        }
      }
    };
  </script>
  <meta name="description" content="背景  项目用到的数据集来自kaggle,共包含1989条记录，每条记录包含每天的Top25条新闻，目标是通过对新闻的词频分析判断股市涨跌。  技术与方法 项目分为四部分：第一部分是解读数据，主要用到Python文本分析工具NLTK中的分词tokenize，需要说明的是，本项目是对英文做分词处理，单词间以空格作为自然分界符，而中文是根据语义做分词，常用工具是结巴分词，得到分词结果后，中英文后续的处">
  <meta name="keywords" content="分词,词频,特征工程,词频统计,TF-IDF,Word2Vec,数据分析,机器学习">
  <meta property="og:type" content="website">
  <meta property="og:title" content="根据新闻内容预测股市动向">
  <meta property="og:url" content="https://zluyang.github.io/backup/根据新闻内容预测股市动向.html">
  <meta property="og:site_name" content="雨生百谷">
  <meta property="og:description" content="背景  项目用到的数据集来自kaggle,共包含1989条记录，每条记录包含每天的Top25条新闻，目标是通过对新闻的词频分析判断股市涨跌。  技术与方法 项目分为四部分：第一部分是解读数据，主要用到Python文本分析工具NLTK中的分词tokenize，需要说明的是，本项目是对英文做分词处理，单词间以空格作为自然分界符，而中文是根据语义做分词，常用工具是结巴分词，得到分词结果后，中英文后续的处">
  <meta property="og:locale" content="zh-Hans">
  <meta property="og:image" content="https://zluyang.github.io/backup/raw_data.jpg">
  <meta property="og:image" content="https://zluyang.github.io/backup/handle_after.jpg">
  <meta property="og:image" content="http://ovbu9b92e.bkt.clouddn.com/stock_predict_01.jpg">
  <meta property="og:image" content="http://ovbu9b92e.bkt.clouddn.com/stock_predict_03.jpg">
  <meta property="og:image" content="http://ovbu9b92e.bkt.clouddn.com/stock_predict_04.jpg">
  <meta property="og:image" content="http://ovbu9b92e.bkt.clouddn.com/stock_predict_05.jpg">
  <meta property="og:updated_time" content="2019-04-27T07:21:20.000Z">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="根据新闻内容预测股市动向">
  <meta name="twitter:description" content="背景  项目用到的数据集来自kaggle,共包含1989条记录，每条记录包含每天的Top25条新闻，目标是通过对新闻的词频分析判断股市涨跌。  技术与方法 项目分为四部分：第一部分是解读数据，主要用到Python文本分析工具NLTK中的分词tokenize，需要说明的是，本项目是对英文做分词处理，单词间以空格作为自然分界符，而中文是根据语义做分词，常用工具是结巴分词，得到分词结果后，中英文后续的处">
  <meta name="twitter:image" content="https://zluyang.github.io/backup/raw_data.jpg">







  <link rel="canonical" href="https://zluyang.github.io/backup/根据新闻内容预测股市动向">



  <script id="page.configurations">
    CONFIG.page = {
      sidebar: "",
    };
  </script>

  <title>根据新闻内容预测股市动向 | 雨生百谷</title>













  <noscript>
    <style>
      .use-motion .motion-element,
      .use-motion .brand,
      .use-motion .menu-item,
      .sidebar-inner,
      .use-motion .post-block,
      .use-motion .pagination,
      .use-motion .comments,
      .use-motion .post-header,
      .use-motion .post-body,
      .use-motion .collection-title {
        opacity: initial;
      }

      .use-motion .logo,
      .use-motion .site-title,
      .use-motion .site-subtitle {
        opacity: initial;
        top: initial;
      }

      .use-motion .logo-line-before i {
        left: initial;
      }

      .use-motion .logo-line-after i {
        right: initial;
      }
    </style>
  </noscript>
</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">






  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner">
        <div class="site-brand-wrapper">
          <div class="site-meta">


            <div class="custom-logo-site-title">
              <a href="/" class="brand" rel="start">
                <span class="logo-line-before"><i></i></span>
                <span class="site-title">雨生百谷</span>
                <span class="logo-line-after"><i></i></span>
              </a>
            </div>


            <p class="site-subtitle">“Live well, Laugh often, Love much.”</p>



          </div>

          <div class="site-nav-toggle">
            <button aria-label="Toggle navigation bar">
              <span class="btn-bar"></span>
              <span class="btn-bar"></span>
              <span class="btn-bar"></span>
            </button>
          </div>
        </div>


        <nav class="site-nav">

          <ul id="menu" class="menu">





            <li class="menu-item menu-item-home">









              <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

            </li>




            <li class="menu-item menu-item-archives">









              <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

            </li>




            <li class="menu-item menu-item-about">









              <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

            </li>




            <li class="menu-item menu-item-tags">









              <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

            </li>




            <li class="menu-item menu-item-categories">









              <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

            </li>



          </ul>






























        </nav>







      </div>
    </header>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">

          <div id="content" class="content">


            <div id="posts" class="posts-expand">



              <div class="post-block page">
                <header class="post-header">

                  <h1 class="post-title" itemprop="name headline">根据新闻内容预测股市动向

                  </h1>

                  <div class="post-meta">





                  </div>

                </header>




                <div class="post-body">


                  <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2>
                  <p> 项目用到的数据集来自<a href="https://www.kaggle.com/aaron7sun/stocknews" target="_blank" rel="noopener">kaggle</a>,共包含1989条记录，每条记录包含每天的Top25条新闻，目标是通过对新闻的词频分析判断股市涨跌。 </p>
                  <h2 id="技术与方法"><a href="#技术与方法" class="headerlink" title="技术与方法"></a>技术与方法</h2>
                  <p>项目分为四部分：<br>第一部分是解读数据，主要用到Python文本分析工具NLTK中的分词tokenize，需要说明的是，本项目是对英文做分词处理，单词间以空格作为自然分界符，而中文是根据语义做分词，常用工具是结巴分词，得到分词结果后，中英文后续的处理是一样的。<br>第二部分是特征工程，包括特征提取、特征选择、特征降维。特征提取使用TF-IDF和Word2Vec结合的方式。TF-IDF是词频-逆文档频率，TF表示某个词在该文件中出现的次数，IDF衡量某个词普遍的重要性。Word2Vec是将语言中字词转为向量形式表达。特征选择使用sklearn中的VarianceThreshold方法,去除方差小的特征。特征降维使用主成分分析PCA方法。<br>第三部分是模型调参及训练，模型调参使用sklearn的网格搜索GridSearchCV方法，模型训练部分选择朴素贝叶斯、逻辑回归、支持向量机SVC和随机森林四种模型。<br>第四部分是模型预测与评价，使用accuracy_score指标和混淆矩阵评价分类准确度。</p>
                  <h2 id="解读数据"><a href="#解读数据" class="headerlink" title="解读数据"></a>解读数据</h2>
                  <h3 id="处理原始数据集"><a href="#处理原始数据集" class="headerlink" title="处理原始数据集"></a>处理原始数据集</h3>
                  <p>读取原始数据集： </p>
                  <pre><code class="python">raw_text_df = pd.read_csv(constant.raw_text_csv_file)
</code></pre>
                  <p><img src="/backup/raw_data.jpg" alt=""><br>第1列是日期，label为0是跌，label为1是涨，后面25列是新闻内容。接下来对原始数据集进行分词处理，这里用到NLTK的分词工具tokenize，和NLTK语料库中的停用词库stopwords，针对原始数据集的每一条新闻做分词处理，去除停用词，得到分词结果后，将分词后的25列新闻合并为1列，存在cln_text_df[‘text’]中，保存处理后的文件cln_text.csv。<br>处理后的数据：
                  </p>
                  <p><img src="/backup/handle_after.jpg" alt=""> </p>
                  <h3 id="分割训练集和测试集"><a href="#分割训练集和测试集" class="headerlink" title="分割训练集和测试集"></a>分割训练集和测试集</h3>
                  <p>读取cln_text.csv,查看总样本数量：<br><img src="http://ovbu9b92e.bkt.clouddn.com/stock_predict_01.jpg" alt=""><br>各类样本数量：<br>label<br>0 924<br>1 1065<br>正负类别样本量均衡，接下来按时间段分割训练集和测试集，训练集时间范围 2008-08-08 ~ 2014-12-31，测试集时间范围 2015-01-02 ~
                    2016-07-01。 </p>
                  <p><img src="http://ovbu9b92e.bkt.clouddn.com/stock_predict_03.jpg" alt=""><br> 得到<br>训练集中各类的数据个数：<br>label<br>0 738<br>1 873<br>测试集中各类的数据个数：<br>label<br>0 186<br>1 192 </p>
                  <h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2>
                  <h3 id="提取文本特征"><a href="#提取文本特征" class="headerlink" title="提取文本特征"></a>提取文本特征</h3>
                  <p>这里选择TF-IDF及word2vec特征,对于word2vec简单使用max pooling方法处理。 </p>
                  <h4 id="训练word2vec模型"><a href="#训练word2vec模型" class="headerlink" title="训练word2vec模型"></a>训练word2vec模型</h4>
                  <p>word2vec可以把字词转为向量形式，参数size是转换后的特征维度(取值100），sg为0时是CBOW模式，为1时是Skip-Gram模式，windows是滑窗尺寸，即寻找相邻词的个数，得到w2v_model.wv即是单词对应的向量。<br><img src="http://ovbu9b92e.bkt.clouddn.com/stock_predict_04.jpg" alt=""></p>
                  <h4 id="在训练集上统计词频"><a href="#在训练集上统计词频" class="headerlink" title="在训练集上统计词频"></a>在训练集上统计词频</h4>
                  <p>将训练集中的单词拿出来放到word_list中，利用NLTK中的词频统计工具FreqDist，统计每个单词出现的次数，这里要注意去除空字符串，否则后续在提取Word2Vec特征时会报错，然后取出现最多的200个词及出现的频率存放在common_words_freqs中：（n_common_words取值200)<br><img src="http://ovbu9b92e.bkt.clouddn.com/stock_predict_05.jpg" alt=""></p>
                  <p>得到结果：出现最多的200个词是：<br>us: 2499次<br>israel: 1948次<br>new: 1872次<br>says: 1841次<br>world: 1829次<br>china: 1729次<br>government: 1724次<br>police: 1666次<br>……</p>
                  <h4 id="在训练集和测试集上提取特征"><a href="#在训练集和测试集上提取特征" class="headerlink" title="在训练集和测试集上提取特征"></a>在训练集和测试集上提取特征</h4>
                  <p>定义提取特征的函数extract_feat_from_data,遍历输入的数据集的每一行单词和高频词数组common_words，如果高频词有出现在输入的文本，则计算该词在这一行文本的TF-IDF值，同时将训练word2vec生成的wv相应词的向量赋值给w2v_val，最终得到存放了TF-IDF值的tf_idf_feat_val_list和存放了word2vec特征的w2v_feat_val_list。<br>对w2v_feat_val_list做max
                    pooling处理，即每列取最大值作为代表特征，再将得到的结果和TF-IDF合并，得到特征矩阵X，每个样本特征维度是300。 </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">    text_collection = TextCollection(train_text_df[<span class="string">'text'</span>].values.tolist())</span><br><span class="line">    train_X, train_y = extract_feat_from_data(train_text_df, text_collection, common_words_freqs) </span><br><span class="line">    test_X, test_y = extract_feat_from_data(test_text_df, text_collection, common_words_freqs)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_feat_from_data</span><span class="params">(text_df, text_collection, common_words_freqs)</span>:</span></span><br><span class="line">    <span class="comment">#特征提取</span></span><br><span class="line">    <span class="comment">#选择TF-IDF及word2vec特征</span></span><br><span class="line">    <span class="comment">#对于word2vec使用max pooling做处理</span></span><br><span class="line">    n_sample = text_df.shape[<span class="number">0</span>]</span><br><span class="line">    n_feat = constant.n_common_words + constant.w2v_dim</span><br><span class="line">    common_words = [word <span class="keyword">for</span> word, _ <span class="keyword">in</span> common_words_freqs]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化</span></span><br><span class="line">    X = np.zeros([n_sample, n_feat])</span><br><span class="line">    y = np.zeros(n_sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, r_data <span class="keyword">in</span> text_df.iterrows():</span><br><span class="line"></span><br><span class="line">        text = r_data[<span class="string">'text'</span>]</span><br><span class="line"></span><br><span class="line">        tf_idf_feat_val_list = []</span><br><span class="line">        w2v_feat_val_list = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> common_words:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> text:</span><br><span class="line">                <span class="comment">#如果在高频词中</span></span><br><span class="line">                <span class="comment">#计算TF-IDF值</span></span><br><span class="line">                tf_idf_val = text_collection.tf_idf(word, text)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#获取word2vec特征</span></span><br><span class="line">                w2v_val = keyed_vectors[word]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tf_idf_val = <span class="number">0</span></span><br><span class="line">                w2v_val = np.zeros(constant.w2v_dim)</span><br><span class="line"></span><br><span class="line">            tf_idf_feat_val_list.append(tf_idf_val)</span><br><span class="line">            w2v_feat_val_list.append(w2v_val)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#max pooling</span></span><br><span class="line">        w2v_feat_val_arr = np.array(w2v_feat_val_list)</span><br><span class="line">        max_pooled_w2v_feat_val_list = np.max(w2v_feat_val_arr, axis=<span class="number">0</span>).tolist()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#赋值</span></span><br><span class="line">        X[i, :] = np.array(tf_idf_feat_val_list + max_pooled_w2v_feat_val_list)</span><br><span class="line">        y[i] = int(r_data[<span class="string">'label'</span>])</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3>
                  <h4 id="特征范围归一化"><a href="#特征范围归一化" class="headerlink" title="特征范围归一化"></a>特征范围归一化</h4>
                  <p>使用sklearn.preprocessing中的StandardScaler工具将特征范围归一化到（-1，1） </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">scaler = StandardScaler()</span><br><span class="line">  tr_feat_scaled = scaler.fit_transform(train_X)</span><br><span class="line">  te_feat_scaled = scaler.transform(test_X)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4>
                  <p>使用sklearn中的特征选择方法VarianceThreshold,去除方差小的特征，这里根据方差保留80%的向量 </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">sel = VarianceThreshold(threshold=(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)))</span><br><span class="line">tr_feat_scaled_sel = sel.fit_transform(tr_feat_scaled)</span><br><span class="line">te_feat_scaled_sel = sel.transform(te_feat_scaled)</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>特征选择后每个样本特征维度： 294</p>
                  <h4 id="特征降维"><a href="#特征降维" class="headerlink" title="特征降维"></a>特征降维</h4>
                  <p>使用sklearn中的PCA实现特征降维，和特征选择不同，PCA通过线型变换将原数据映射到新的坐标系统中，使映射后的第一个坐标上的方差最大（即第一个主成分），依次递减。这里保留95%贡献率的特征向量。 </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">pca = PCA(n_components=<span class="number">0.95</span>)  <span class="comment"># 保留95%贡献率的特征向量</span></span><br><span class="line">tr_feat_scaled_sel_pca = pca.fit_transform(tr_feat_scaled_sel)</span><br><span class="line">te_feat_scaled_sel_pca = pca.transform(te_feat_scaled_sel)</span><br><span class="line">print(<span class="string">'处理后每个样本特征维度：'</span>, tr_feat_scaled_sel_pca.shape[<span class="number">1</span>])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>处理后每个样本特征维度： 192</p>
                  <h2 id="模型调参及训练"><a href="#模型调参及训练" class="headerlink" title="模型调参及训练"></a>模型调参及训练</h2>
                  <p>项目要解决的是分类问题，股市涨跌属于二分类问题，这里选择4种机器学习模型进行预测：朴素贝叶斯，逻辑回归，支持向量机，随机森林。其中朴素贝叶斯不需要调参，其他三个模型都需要参数调优。</p>
                  <h3 id="训练朴素贝叶斯模型"><a href="#训练朴素贝叶斯模型" class="headerlink" title="训练朴素贝叶斯模型"></a>训练朴素贝叶斯模型</h3>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">models = []</span><br><span class="line">print(<span class="string">'1. 朴素贝叶斯模型：'</span>)</span><br><span class="line">gnb_model = GaussianNB()</span><br><span class="line">gnb_model.fit(tr_feat_scaled_sel_pca, train_y)</span><br><span class="line">models.append([<span class="string">'朴素贝叶斯'</span>, gnb_model])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <h3 id="训练逻辑回归模型"><a href="#训练逻辑回归模型" class="headerlink" title="训练逻辑回归模型"></a>训练逻辑回归模型</h3>
                  <p>逻辑回归为了平衡损失函数和正则项的关系，引入了损失函数的系数C作为模型超参数，C越大，损失函数的调节越重要，C越小，正则项的调节越重要。这里对C选取5个值，使用sklearn的网格搜索GridSearchCV进行交叉验证获取最优参数。 </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">    print(<span class="string">'2. 逻辑回归：'</span>)</span><br><span class="line">    lr_param_grid = [</span><br><span class="line">        &#123;<span class="string">'C'</span>: [<span class="number">1e-3</span>, <span class="number">1e-2</span>, <span class="number">1e-1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]&#125;</span><br><span class="line">    ]</span><br><span class="line">    lr_model = LogisticRegression()</span><br><span class="line">    best_lr_model = get_best_model(lr_model,</span><br><span class="line">                                   tr_feat_scaled_sel_pca, train_y,</span><br><span class="line">                                   lr_param_grid, cv=<span class="number">3</span>)</span><br><span class="line">    models.append([<span class="string">'逻辑回归'</span>, best_lr_model])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_best_model</span><span class="params">(model, X_train, y_train, params, cv)</span>:</span></span><br><span class="line">    <span class="comment">#交叉验证获取最优模型</span></span><br><span class="line">    clf = GridSearchCV(model, params, cv=cv, verbose=<span class="number">3</span>)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> clf.best_estimator_</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>逻辑回归最佳参数： C=10</p>
                  <h3 id="训练SVC模型"><a href="#训练SVC模型" class="headerlink" title="训练SVC模型"></a>训练SVC模型</h3>
                  <p>这里对SVC的3个参数进行调优，惩罚因子C：C依然是为了平衡损失函数和正则项的关系，与逻辑回归的C不同的是，SVC的C是正则项的系数，C越大，模型的容错空间越小，越容易过拟合，C越小，模型的容错空间越大，越容易欠拟合；kernel选择高斯核函数，gamma是高斯核函数的系数，gamma越大，高斯分布越窄，越容易过拟合，gamma越小，高斯分布越宽，越容易欠拟合，这里gamma分别取0.001和0.0001。 </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">print(<span class="string">'3. 支持向量机：'</span>)</span><br><span class="line">svm_param_grid = [</span><br><span class="line">    &#123;<span class="string">'C'</span>: [<span class="number">1e-2</span>, <span class="number">1e-1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>], <span class="string">'gamma'</span>: [<span class="number">0.001</span>, <span class="number">0.0001</span>], <span class="string">'kernel'</span>: [<span class="string">'rbf'</span>]&#125;,</span><br><span class="line">]</span><br><span class="line">svm_model = svm.SVC(probability=<span class="keyword">True</span>)</span><br><span class="line">best_svm_model = get_best_model(svm_model,</span><br><span class="line">                                tr_feat_scaled_sel_pca, train_y,</span><br><span class="line">                                svm_param_grid, cv=<span class="number">3</span>)</span><br><span class="line">models.append([<span class="string">'支持向量机'</span>, best_svm_model])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>SVC的最佳参数是：C=0.01,gamma=0.001，kernel=’rbf’</p>
                  <h3 id="训练随机森林模型"><a href="#训练随机森林模型" class="headerlink" title="训练随机森林模型"></a>训练随机森林模型</h3>
                  <p>这里选择n_estimators作为调参对象，n_estimators是子模型数量，在临界值内，数量越大，效果越好，但是计算时间也会随之增加。 </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line">print(<span class="string">'4. 随机森林：'</span>)</span><br><span class="line">rf_param_grid = [</span><br><span class="line">    &#123;<span class="string">'n_estimators'</span>: [<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">150</span>, <span class="number">200</span>]&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">rf_model = RandomForestClassifier()</span><br><span class="line">best_rf_model = get_best_model(rf_model,</span><br><span class="line">                               tr_feat_scaled_sel_pca, train_y,</span><br><span class="line">                               rf_param_grid, cv=<span class="number">3</span>)</span><br><span class="line">rf_model.fit(tr_feat_scaled_sel_pca, train_y)</span><br><span class="line">models.append([<span class="string">'随机森林'</span>, best_rf_model])</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>随机森林最佳参数： n_estimators=50</p>
                  <h3 id="模型预测与评价"><a href="#模型预测与评价" class="headerlink" title="模型预测与评价"></a>模型预测与评价</h3>
                  <p>使用sklearn的accuracy_score评价分类准确度，得分越高，说明预测准确度越高；<br>最后输出每个模型的混淆矩阵。 </p>
                  <figure class="highlight python">
                    <table>
                      <tr>
                        <td class="gutter">
                          <pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre>
                        </td>
                        <td class="code">
                          <pre><span class="line"><span class="keyword">for</span> i, model <span class="keyword">in</span> enumerate(models):</span><br><span class="line">    print(<span class="string">'&#123;&#125;-&#123;&#125;'</span>.format(i + <span class="number">1</span>, model[<span class="number">0</span>]))</span><br><span class="line">    <span class="comment">#输出准确率</span></span><br><span class="line">    print(<span class="string">'准确率：'</span>, accuracy_score(test_y, model[<span class="number">1</span>].predict(te_feat_scaled_sel_pca)))</span><br><span class="line">    <span class="comment">#输出混淆矩阵</span></span><br><span class="line">    print(<span class="string">'混淆矩阵'</span>)</span><br><span class="line">    print(confusion_matrix(test_y, model[<span class="number">1</span>].predict(te_feat_scaled_sel_pca)))</span><br></pre>
                        </td>
                      </tr>
                    </table>
                  </figure>
                  <p>1-朴素贝叶斯<br>准确率： 0.510582010582<br>混淆矩阵<br>[[ 64 122]<br> [ 63 129]] </p>
                  <p>2-逻辑回归<br>准确率： 0.502645502646<br>混淆矩阵<br>[[ 71 115]<br> [ 73 119]] </p>
                  <p>3-支持向量机<br>准确率： 0.507936507937<br>混淆矩阵<br>[[ 0 186]<br> [ 0 192]] </p>
                  <p>4-随机森林<br>准确率： 0.502645502646<br>混淆矩阵<br>[[ 40 146]<br> [ 42 150]] </p>
                  <p>可以看出，朴素贝叶斯准确率最高，SVC在预测跌势的能力最差，朴素贝叶斯、逻辑回归和随机森林在预测涨势时比跌势更准确。</p>

                </div>



              </div>







            </div>


          </div>



          <div class="comments" id="comments">
          </div>





        </div>



        <div class="sidebar-toggle">
          <div class="sidebar-toggle-line-wrap">
            <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
            <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
            <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
          </div>
        </div>

        <aside id="sidebar" class="sidebar">
          <div class="sidebar-inner">




            <ul class="sidebar-nav motion-element">
              <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
                Table of Contents
              </li>
              <li class="sidebar-nav-overview" data-target="site-overview-wrap">
                Overview
              </li>
            </ul>


            <div class="site-overview-wrap sidebar-panel">
              <div class="site-overview">
                <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">

                  <p class="site-author-name" itemprop="name">六郎</p>
                  <div class="site-description motion-element" itemprop="description">“Live well, Laugh often, Love much.”</div>
                </div>


                <nav class="site-state motion-element">

                  <div class="site-state-item site-state-posts">

                    <a href="/archives/">

                      <span class="site-state-item-count">14</span>
                      <span class="site-state-item-name">posts</span>
                    </a>
                  </div>





                  <div class="site-state-item site-state-categories">


                    <a href="/categories/">












                      <span class="site-state-item-count">4</span>
                      <span class="site-state-item-name">categories</span>
                    </a>
                  </div>





                  <div class="site-state-item site-state-tags">


                    <a href="/tags/">




































                      <span class="site-state-item-count">16</span>
                      <span class="site-state-item-name">tags</span>
                    </a>
                  </div>

                </nav>


















              </div>
            </div>


            <!--noindex-->
            <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
              <div class="post-toc">







                <div class="post-toc-content">
                  <ol class="nav">
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#背景"><span class="nav-number">1.</span> <span class="nav-text">背景</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#技术与方法"><span class="nav-number">2.</span> <span class="nav-text">技术与方法</span></a></li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#解读数据"><span class="nav-number">3.</span> <span class="nav-text">解读数据</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#处理原始数据集"><span class="nav-number">3.1.</span> <span class="nav-text">处理原始数据集</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#分割训练集和测试集"><span class="nav-number">3.2.</span> <span class="nav-text">分割训练集和测试集</span></a></li>
                      </ol>
                    </li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#特征工程"><span class="nav-number">4.</span> <span class="nav-text">特征工程</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#提取文本特征"><span class="nav-number">4.1.</span> <span class="nav-text">提取文本特征</span></a>
                          <ol class="nav-child">
                            <li class="nav-item nav-level-4"><a class="nav-link" href="#训练word2vec模型"><span class="nav-number">4.1.1.</span> <span class="nav-text">训练word2vec模型</span></a></li>
                            <li class="nav-item nav-level-4"><a class="nav-link" href="#在训练集上统计词频"><span class="nav-number">4.1.2.</span> <span class="nav-text">在训练集上统计词频</span></a></li>
                            <li class="nav-item nav-level-4"><a class="nav-link" href="#在训练集和测试集上提取特征"><span class="nav-number">4.1.3.</span> <span class="nav-text">在训练集和测试集上提取特征</span></a></li>
                          </ol>
                        </li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#特征处理"><span class="nav-number">4.2.</span> <span class="nav-text">特征处理</span></a>
                          <ol class="nav-child">
                            <li class="nav-item nav-level-4"><a class="nav-link" href="#特征范围归一化"><span class="nav-number">4.2.1.</span> <span class="nav-text">特征范围归一化</span></a></li>
                            <li class="nav-item nav-level-4"><a class="nav-link" href="#特征选择"><span class="nav-number">4.2.2.</span> <span class="nav-text">特征选择</span></a></li>
                            <li class="nav-item nav-level-4"><a class="nav-link" href="#特征降维"><span class="nav-number">4.2.3.</span> <span class="nav-text">特征降维</span></a></li>
                          </ol>
                        </li>
                      </ol>
                    </li>
                    <li class="nav-item nav-level-2"><a class="nav-link" href="#模型调参及训练"><span class="nav-number">5.</span> <span class="nav-text">模型调参及训练</span></a>
                      <ol class="nav-child">
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#训练朴素贝叶斯模型"><span class="nav-number">5.1.</span> <span class="nav-text">训练朴素贝叶斯模型</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#训练逻辑回归模型"><span class="nav-number">5.2.</span> <span class="nav-text">训练逻辑回归模型</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#训练SVC模型"><span class="nav-number">5.3.</span> <span class="nav-text">训练SVC模型</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#训练随机森林模型"><span class="nav-number">5.4.</span> <span class="nav-text">训练随机森林模型</span></a></li>
                        <li class="nav-item nav-level-3"><a class="nav-link" href="#模型预测与评价"><span class="nav-number">5.5.</span> <span class="nav-text">模型预测与评价</span></a></li>
                      </ol>
                    </li>
                  </ol>
                </div>


              </div>
            </div>
            <!--/noindex-->




          </div>
        </aside>




      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
          <span class="with-love" id="animate">
            <i class="fa fa-user"></i>
          </span>
          <span class="author" itemprop="copyrightHolder">六郎</span>




        </div>


        <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



        <span class="post-meta-divider">|</span>



        <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.1.0</div>










      </div>
    </footer>


    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>

    </div>







  </div>



  <script>
    if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
      window.Promise = null;
    }
  </script>



























  <script src="/lib/jquery/index.js?v=2.1.3"></script>


  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>


  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>





  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>







  <script src="/js/schemes/muse.js?v=7.1.0"></script>




  <script src="/js/scrollspy.js?v=7.1.0"></script>
  <script src="/js/post-details.js?v=7.1.0"></script>






  <script src="/js/next-boot.js?v=7.1.0"></script>











  <script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>

  <script>
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(function(item) {
      return GUEST.indexOf(item) > -1;
    });
    new Valine({
      el: '#comments',
      verify: false,
      notify: false,
      appId: 'cpCz4RKVyMs2VawqHCdUHBmM-gzGzoHsz',
      appKey: 'BVemUJYa8azbst7hlR00hCkn',
      placeholder: '请在此输入您的留言',
      avatar: 'mm',
      meta: guest,
      pageSize: '10' || 10,
      visitor: ,
      lang: '' || 'zh-cn'
    });
  </script>














  <script>
    function showTime(Counter) {
      var entries = [];
      var $visitors = $('.leancloud_visitors');

      $visitors.each(function() {
        entries.push($(this).attr('id').trim());
      });

      Counter('get', '/classes/Counter', {
          where: JSON.stringify({
            url: {
              '$in': entries
            }
          })
        })
        .done(function({
          results
        }) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if (countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function({
          responseJSON
        }) {
          console.log('LeanCloud Counter Error: ' + responseJSON.code + ' ' + responseJSON.error);
        });
    }


    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + 'cpCz4RKVyMs2VawqHCdUHBmM-gzGzoHsz')
        .done(function({
          api_server
        }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': 'cpCz4RKVyMs2VawqHCdUHBmM-gzGzoHsz',
                'X-LC-Key': 'BVemUJYa8azbst7hlR00hCkn',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };

          if ($('.post-title-link').length >= 1) {
            showTime(Counter);
          }

        });
    });
  </script>





























</body>

</html>